# 1. IRIS RAG Demo

![IRIS RAG Demo](https://github.com/grongierisc/iris-rag-demo/blob/master/misc/title.jpg?raw=true)

Ceci est une simple d√©mo de l'IRIS avec un exemple de RAG (Retrieval Augmented Generation).
Le backend est √©crit en Python en utilisant IRIS et IoP, le mod√®le LLM est `orca-mini` et est servi par le serveur `ollama`.
Le frontend est un chatbot √©crit avec Streamlit.

- [1. IRIS RAG Demo](#1-iris-rag-demo)
  - [1.1. Quest-ce que RAG?](#11-quest-ce-que-rag)
  - [1.2. Comment √ßa marche?](#12-comment-√ßa-marche)
  - [1.3. Installation de la d√©mo](#13-installation-de-la-d√©mo)
  - [1.4. Usage](#14-usage)
  - [1.5. Comment fonctionne la d√©mo ?](#15-comment-fonctionne-la-d√©mo-)
    - [1.5.1. Le frontend](#151-le-frontend)
    - [1.5.2. Le backend](#152-le-backend)
      - [1.5.2.1. Le business service](#1521-le-business-service)
      - [1.5.2.2. Le business process](#1522-le-business-process)
      - [1.5.2.3. L'op√©ration LLM](#1523-lop√©ration-llm)
      - [1.5.2.4. L'op√©ration vectorielle](#1524-lop√©ration-vectorielle)
  - [1.6. Remarques g√©n√©rales](#16-remarques-g√©n√©rales)


## 1.1. Quest-ce que RAG?

RAG signifie Retrieval Augmented Generation, il permet d'utiliser un mod√®le LLM (GPT-3.5/4, Mistral, Orca, etc.) avec une **base de connaissances**.

**Pourquoi est-ce important ?** Parce que cela permet d'utiliser une *base de connaissances* pour r√©pondre aux questions, et d'utiliser le LLM pour g√©n√©rer la r√©ponse.

Par exemple, si vous demandez **"Qu'est-ce que le module grongier.pex ?"** directement au LLM, il ne pourra pas r√©pondre, car il ne sait pas ce qu'est ce module (et peut-√™tre que vous ne le savez pas non plus ü§™).

Mais si vous posez la m√™me question √† RAG, il pourra r√©pondre, car il utilisera la *base de connaissances* qui sait ce qu'est le module grongier.pex pour trouver la r√©ponse.

Maintenant que vous savez ce qu'est RAG, voyons comment cela fonctionne.

## 1.2. Comment √ßa marche?

Tout d'abord, nous devons comprendre comment fonctionne un LLM. Les LLM sont entra√Æn√©s pour pr√©dire le mot suivant, √©tant donn√© les mots pr√©c√©dents. Ainsi, si vous lui donnez une phrase, il essaiera de pr√©dire le mot suivant, et ainsi de suite. Facile, non ?

Pour interagir avec un LLM, vous devez g√©n√©ralement lui donner une requ√™te, et il g√©n√©rera le reste de la phrase. Par exemple, si vous lui donnez la requ√™te `Qu'est-ce que le module grongier.pex ?`, il g√©n√©rera le reste de la phrase, et cela ressemblera √† ceci :

```
Je suis d√©sol√©, mais je ne connais pas le module Pex que vous avez mentionn√©. Pouvez-vous fournir plus d'informations ou de contexte √† ce sujet ?
```

Ok, comme pr√©vu, il ne sait pas ce qu'est le module grongier.pex. Mais que se passe-t-il si nous lui donnons une requ√™te qui contient la r√©ponse ? Par exemple, si nous lui donnons la requ√™te `Qu'est-ce que le module grongier.pex ? C'est un module qui vous permet de faire X, Y et Z.`, il g√©n√©rera le reste de la phrase, et cela ressemblera √† ceci :

```
Le module grongier.pex est un module qui vous permet de faire X, Y et Z.
```

Ok, maintenant il sait ce qu'est le module grongier.pex.

Mais que se passe-t-il si nous ne savons pas ce qu'est le module grongier.pex ? Comment pouvons-nous lui donner une requ√™te qui contient la r√©ponse ?
Eh bien, c'est l√† que la *base de connaissances* entre en jeu.

![RAG](https://github.com/grongierisc/iris-rag-demo/blob/master/misc/rag_schema.png?raw=true)

L'id√©e de RAG est d'utiliser la *base de connaissances* pour trouver le **contexte**, puis d'utiliser le LLM pour g√©n√©rer la r√©ponse.

Pour trouver le **contexte**, RAG utilisera un **retriever**. Le **retriever** recherchera la *base de connaissances* pour les documents les plus pertinents, puis RAG utilisera le LLM pour g√©n√©rer la r√©ponse.

Pour rechercher la *base de connaissances*, nous utiliserons la recherche vectorielle.

La recherche vectorielle est une technique qui permet de trouver les documents les plus pertinents √©tant donn√© une requ√™te. Elle fonctionne en convertissant les documents et la requ√™te en vecteurs, puis en calculant la similarit√© cosinus entre le vecteur de la requ√™te et les vecteurs des documents. Plus la similarit√© cosinus est √©lev√©e, plus le document est pertinent.

Pour plus d'informations sur la recherche vectorielle, vous pouvez consulter [ce lien](https://community.intersystems.com/post/vectors-support-well-almost). Merci √† @Dmitry Maslennikov pour son article.

![Vector Search](https://github.com/grongierisc/iris-rag-demo/blob/master/misc/vector_search.jpg?raw=true)

Maintenant que nous savons comment fonctionne RAG, voyons comment l'utiliser.

## 1.3. Installation de la d√©mo

Pour installer la d√©mo, vous devez avoir Docker et Docker Compose install√©s sur votre machine.

Ensuite, il suffit de cloner le repo et d'ex√©cuter la commande `docker-compose up`.

```bash
git clone https://github.com/grongierisc/iris-rag-demo
cd iris-rag-demo
docker-compose up
```

‚ö†Ô∏è tout est local, rien n'est envoy√© dans le cloud, donc soyez patient, cela peut prendre quelques minutes pour d√©marrer.  

## 1.4. Usage

Une fois la d√©mo d√©marr√©e, vous pouvez acc√©der au frontend √† l'adresse http://localhost:8501.

![Frontend](https://github.com/grongierisc/iris-rag-demo/blob/master/misc/iris_chat.png?raw=true)

Vous pouvez poser des questions sur l'IRIS, par exemple :

- Qu'est-ce que le module grongier.pex ?

![Question](https://github.com/grongierisc/iris-rag-demo/blob/master/misc/without_rag.png?raw=true)

Comme vous pouvez le voir, la r√©ponse n'est pas tr√®s bonne, car le LLM ne sait pas ce qu'est le module grongier.pex.

Maintenant, essayons avec RAG :

Uploader la documentation du module `grongier.pex`, elle se trouve dans le dossier `docs`, fichier `grongier.pex.md`.

Ensuite, posez la m√™me question :

- Qu'est-ce que le module grongier.pex ?

![Question](https://github.com/grongierisc/iris-rag-demo/blob/master/misc/with_rag.png?raw=true)

Comme vous pouvez le voir, la r√©ponse est bien meilleure, car le LLM sait maintenant ce qu'est le module grongier.pex.

Vous pouvez voir les d√©tails dans les logs :

Aller dans le portail de gestion √† l'adresse http://localhost:53795/csp/irisapp/EnsPortal.ProductionConfig.zen?$NAMESPACE=IRISAPP&$NAMESPACE=IRISAPP& et cliquer sur l'onglet `Messages`.

Premi√®rement, vous verrez le message envoy√© au processus RAG :

![Message](https://github.com/grongierisc/iris-rag-demo/blob/master/misc/trace_query.png?raw=true)

Ensuite, la requ√™te de recherche dans la *base de connaissances* (base de donn√©es vectorielle) :

![Message](https://github.com/grongierisc/iris-rag-demo/blob/master/misc/trace_result_vector.png?raw=true)

Et enfin la nouvelle requ√™te envoy√©e au LLM :

![Message](https://github.com/grongierisc/iris-rag-demo/blob/master/misc/trace_new_query.png?raw=true)

## 1.5. Comment fonctionne la d√©mo ?

La d√©mo est compos√©e de 3 parties :

- Le frontend, √©crit avec Streamlit
- Le backend, √©crit avec Python et IRIS
- La *base de connaissances* Chroma et la base de donn√©es vectorielle
- Le LLM, Orca-mini, servi par le serveur Ollama

### 1.5.1. Le frontend

Le frontend est √©crit avec Streamlit, c'est un simple chatbot qui vous permet de poser des questions.

Rien de bien compliqu√© ici, juste un simple chatbot.

<spoiler>

```python
import os
import tempfile
import time
import streamlit as st
from streamlit_chat import message

from grongier.pex import Director

_service = Director.create_python_business_service("ChatService")

st.set_page_config(page_title="ChatIRIS")


def display_messages():
    st.subheader("Chat")
    for i, (msg, is_user) in enumerate(st.session_state["messages"]):
        message(msg, is_user=is_user, key=str(i))


def process_input():
    if st.session_state["user_input"] and len(st.session_state["user_input"].strip()) > 0:
        user_text = st.session_state["user_input"].strip()
        with st.spinner(f"Thinking about {user_text}"):
            rag_enabled = False
            if len(st.session_state["file_uploader"]) > 0:
                rag_enabled = True
            time.sleep(1) # help the spinner to show up
            agent_text = _service.ask(user_text, rag_enabled)

        st.session_state["messages"].append((user_text, True))
        st.session_state["messages"].append((agent_text, False))


def read_and_save_file():

    for file in st.session_state["file_uploader"]:
        with tempfile.NamedTemporaryFile(delete=False,suffix=f".{file.name.split('.')[-1]}") as tf:
            tf.write(file.getbuffer())
            file_path = tf.name

        with st.spinner(f"Ingesting {file.name}"):
            _service.ingest(file_path)
        os.remove(file_path)

    if len(st.session_state["file_uploader"]) > 0:
        st.session_state["messages"].append(
            ("File(s) successfully ingested", False)
        )

    if len(st.session_state["file_uploader"]) == 0:
        _service.clear()
        st.session_state["messages"].append(
            ("Clearing all data", False)
        )

def page():
    if len(st.session_state) == 0:
        st.session_state["messages"] = []
        _service.clear()

    st.header("ChatIRIS")

    st.subheader("Upload a document")
    st.file_uploader(
        "Upload document",
        type=["pdf", "md", "txt"],
        key="file_uploader",
        on_change=read_and_save_file,
        label_visibility="collapsed",
        accept_multiple_files=True,
    )

    display_messages()
    st.text_input("Message", key="user_input", on_change=process_input)


if __name__ == "__main__":
    page()
```
</spoiler>

üí° Je n'utilise que :

```python
_service = Director.create_python_business_service("ChatService")
```

Pour cr√©er un lien entre le frontend et le backend.

`ChatService` est un simple service m√©tier dans la production d'interop√©rabilit√©.

### 1.5.2. Le backend

Le backend est √©crit avec Python et IRIS.

Il est compos√© de 3 parties :

- Le service m√©tier
  - point d'entr√©e du frontend
- Le processus m√©tier
  - effectuer la recherche dans la *base de connaissances* si n√©cessaire
- Deux op√©rations m√©tier
  - Une pour la *base de connaissances*
    - Ingestion des documents
    - Recherche des documents
    - Effacer les documents
  - Une pour le LLM
    - G√©n√©rer la r√©ponse

#### 1.5.2.1. Le business service

Le service m√©tier est un simple service m√©tier qui permet :
- D'uploader des documents
- De poser des questions
- De vider la base de donn√©es vectorielle

<spoiler>

```python
from grongier.pex import BusinessService

from rag.msg import ChatRequest, ChatClearRequest, FileIngestionRequest

class ChatService(BusinessService):

    def on_init(self):
        if not hasattr(self, "target_chat"):
            self.target_chat = "ChatProcess"
        if not hasattr(self, "target_vector"):
            self.target_vector = "VectorOperation"

    def ingest(self, file_path: str):
        # build message
        msg = FileIngestionRequest(file_path=file_path)
        # send message
        self.send_request_sync(self.target_vector, msg)

    def ask(self, query: str, rag: bool = False):
        # build message
        msg = ChatRequest(query=query)
        # send message
        response = self.send_request_sync(self.target_chat, msg)
        # return response
        return response.response

    def clear(self):
        # build message
        msg = ChatClearRequest()
        # send message
        self.send_request_sync(self.target_vector, msg)
```
</spoiler>

Si vous regardez le code, vous verrez que le service m√©tier est tr√®s simple, il ne fait que passer entre l'op√©ration et le processus.

#### 1.5.2.2. Le business process

Le processus m√©tier est aussi un simple processus qui permet de rechercher la *base de connaissances* si n√©cessaire.

<spoiler>

```python
from grongier.pex import BusinessProcess

from rag.msg import ChatRequest, ChatResponse, VectorSearchRequest

class ChatProcess(BusinessProcess):
    """
    the aim of this process is to generate a prompt from a query
    if the vector similarity search returns a document, then we use the document's content as the prompt
    if the vector similarity search returns nothing, then we use the query as the prompt
    """
    def on_init(self):
        if not hasattr(self, "target_vector"):
            self.target_vector = "VectorOperation"
        if not hasattr(self, "target_chat"):
            self.target_chat = "ChatOperation"

        # prompt template
        self.prompt_template = "Given the context: \n {context} \n Answer the question: {question}"


    def ask(self, request: ChatRequest):
        query = request.query
        prompt = ""
        # build message
        msg = VectorSearchRequest(query=query)
        # send message
        response = self.send_request_sync(self.target_vector, msg)
        # if we have a response, then use the first document's content as the prompt
        if response.docs:
            # add each document's content to the context
            context = "\n".join([doc['page_content'] for doc in response.docs])
            # build the prompt
            prompt = self.prompt_template.format(context=context, question=query)
        else:
            # use the query as the prompt
            prompt = query
        # build message
        msg = ChatRequest(query=prompt)
        # send message
        response = self.send_request_sync(self.target_chat, msg)
        # return response
        return response
```
</spoiler>

Comme je le disais, le processus est tr√®s simple, il ne fait que passer entre l'op√©ration et le processus.

Si la recherche vectorielle retourne des documents, alors il utilisera le contenu des documents comme prompt, sinon il utilisera la requ√™te comme prompt.

#### 1.5.2.3. L'op√©ration LLM

L'op√©ration LLM est une simple op√©ration qui permet de g√©n√©rer la r√©ponse.

<spoiler>

```python

class ChatOperation(BusinessOperation):

    def __init__(self):
        self.model = None

    def on_init(self):
        self.model = Ollama(base_url="http://ollama:11434",model="orca-mini")

    def ask(self, request: ChatRequest):
        return ChatResponse(response=self.model(request.query))
```

</spoiler>

Difficile de faire plus simple, non ?

#### 1.5.2.4. L'op√©ration vectorielle

L'op√©ration vectorielle est une op√©ration qui permet d'ing√©rer des documents, de rechercher des documents et de vider la base de donn√©es vectorielle.

<spoiler>

```python

class VectorOperation(BusinessOperation):

    def __init__(self):
        self.text_splitter = None
        self.vector_store = None

    def on_init(self):
        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=100)
        self.vector_store = Chroma(persist_directory="vector",embedding_function=FastEmbedEmbeddings())

    def ingest(self, request: FileIngestionRequest):
        file_path = request.file_path
        file_type = self._get_file_type(file_path)
        if file_type == "pdf":
            self._ingest_pdf(file_path)
        elif file_type == "markdown":
            self._ingest_markdown(file_path)
        elif file_type == "text":
            self._ingest_text(file_path)
        else:
            raise Exception(f"Unknown file type: {file_type}")

    def clear(self, request: ChatClearRequest):
        self.on_tear_down()

    def similar(self, request: VectorSearchRequest):
        # do a similarity search
        docs = self.vector_store.similarity_search(request.query)
        # return the response
        return VectorSearchResponse(docs=docs)

    def on_tear_down(self):
        docs = self.vector_store.get()
        for id in docs['ids']:
            self.vector_store.delete(id)
        
    def _get_file_type(self, file_path: str):
        if file_path.lower().endswith(".pdf"):
            return "pdf"
        elif file_path.lower().endswith(".md"):
            return "markdown"
        elif file_path.lower().endswith(".txt"):
            return "text"
        else:
            return "unknown"

    def _store_chunks(self, chunks):
        ids = [str(uuid.uuid5(uuid.NAMESPACE_DNS, doc.page_content)) for doc in chunks]
        unique_ids = list(set(ids))
        self.vector_store.add_documents(chunks, ids = unique_ids)
        
    def _ingest_text(self, file_path: str):
        docs = TextLoader(file_path).load()
        chunks = self.text_splitter.split_documents(docs)
        chunks = filter_complex_metadata(chunks)

        self._store_chunks(chunks)
        
    def _ingest_pdf(self, file_path: str):
        docs = PyPDFLoader(file_path=file_path).load()
        chunks = self.text_splitter.split_documents(docs)
        chunks = filter_complex_metadata(chunks)

        self._store_chunks(chunks)

    def _ingest_markdown(self, file_path: str):
        # Document loader
        docs = TextLoader(file_path).load()

        # MD splits
        headers_to_split_on = [
            ("#", "Header 1"),
            ("##", "Header 2"),
        ]

        markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
        md_header_splits = markdown_splitter.split_text(docs[0].page_content)

        # Split
        chunks = self.text_splitter.split_documents(md_header_splits)
        chunks = filter_complex_metadata(chunks)

        self._store_chunks(chunks)
```

</spoiler>

Si vous regardez le code, vous verrez que l'op√©ration vectorielle est un peu plus complexe que les autres.
Les raisons sont les suivantes :

- Nous devons ing√©rer des documents
- Nous devons rechercher des documents
- Nous devons vider la base de donn√©es vectorielle

Pour ing√©rer des documents, nous devons d'abord les charger, puis les diviser en morceaux, puis les stocker dans la base de donn√©es vectorielle.

Le processus de diviser est **important**, car cela permettra √† la recherche vectorielle de trouver les documents les plus pertinents.

Par exemple, si nous avons un document qui contient 1000 mots, et que nous le divisons en 10 morceaux de 100 mots, alors la recherche vectorielle pourra trouver les documents les plus pertinents, car elle pourra comparer les vecteurs de la requ√™te avec les vecteurs des morceaux.

Dans le cas des markdowns, nous utilisons √©galement les en-t√™tes pour diviser le document en morceaux.

## 1.6. Remarques g√©n√©rales

Tout cela peut √™tre fait avec `langchains`, mais je voulais vous montrer comment le faire avec le framework d'interop√©rabilit√©. Et le rendre plus accessible √† tous pour comprendre comment le principe des RAG fonctionne.
